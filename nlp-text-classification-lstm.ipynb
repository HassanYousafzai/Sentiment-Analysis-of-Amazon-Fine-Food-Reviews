{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":4140,"sourceType":"datasetVersion","datasetId":2477}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T05:28:44.099029Z","iopub.execute_input":"2024-06-21T05:28:44.099916Z","iopub.status.idle":"2024-06-21T05:28:46.139440Z","shell.execute_reply.started":"2024-06-21T05:28:44.099865Z","shell.execute_reply":"2024-06-21T05:28:46.138429Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Importing Dependencies","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\nimport re\n\nprint(\"Tensorflow version\", tf.__version__)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:28:46.141714Z","iopub.execute_input":"2024-06-21T05:28:46.142556Z","iopub.status.idle":"2024-06-21T05:29:08.486554Z","shell.execute_reply.started":"2024-06-21T05:28:46.142518Z","shell.execute_reply":"2024-06-21T05:29:08.485590Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv', \n                 encoding = 'latin',header=None)\n                \ndf.sample(6).style.set_properties(**{'background-color': '#f9f9f9', 'color': \n                                     '#4CAF50', 'font-weight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:08.487758Z","iopub.execute_input":"2024-06-21T05:29:08.488310Z","iopub.status.idle":"2024-06-21T05:29:15.080107Z","shell.execute_reply.started":"2024-06-21T05:29:08.488279Z","shell.execute_reply":"2024-06-21T05:29:15.079122Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Giving names to the Columns\ndf.columns = ['sentiment', 'id', 'date', 'query', 'user_id', 'text']\n              \ndf.head().style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:15.081290Z","iopub.execute_input":"2024-06-21T05:29:15.081840Z","iopub.status.idle":"2024-06-21T05:29:15.093464Z","shell.execute_reply.started":"2024-06-21T05:29:15.081812Z","shell.execute_reply":"2024-06-21T05:29:15.092439Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#We only need text to train to classify sentiment so we will drop other columns\ndf = df.drop( ['id', 'date', 'query', 'user_id'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:15.096832Z","iopub.execute_input":"2024-06-21T05:29:15.097186Z","iopub.status.idle":"2024-06-21T05:29:15.140639Z","shell.execute_reply.started":"2024-06-21T05:29:15.097154Z","shell.execute_reply":"2024-06-21T05:29:15.139789Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lab_to_sentiment = {0:\"Negative\", 4:\"Positive\"}\ndef label_decoder(label):\n    return lab_to_sentiment[label]\ndf.sentiment = df.sentiment.apply(lambda x : label_decoder(x))\ndf.head().style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'})","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:15.141908Z","iopub.execute_input":"2024-06-21T05:29:15.142248Z","iopub.status.idle":"2024-06-21T05:29:15.754953Z","shell.execute_reply.started":"2024-06-21T05:29:15.142213Z","shell.execute_reply":"2024-06-21T05:29:15.754048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We have decoded the labels 0 ---> Negative and 1---> Positive as directed by the dataset description. Now we will analyse the dataset for its distribution.","metadata":{}},{"cell_type":"code","source":"val_counts = df.sentiment.value_counts()\n\nplt.figure(figsize=(8,4))\nplt.bar(val_counts.index, val_counts.values)\nplt.title(\"Sentiment Data Distribution\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:15.755987Z","iopub.execute_input":"2024-06-21T05:29:15.756265Z","iopub.status.idle":"2024-06-21T05:29:16.266838Z","shell.execute_reply.started":"2024-06-21T05:29:15.756240Z","shell.execute_reply":"2024-06-21T05:29:16.265746Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Its a very good Dataset without any skewness.\nNow let is explore the data we have","metadata":{}},{"cell_type":"code","source":"import random\nrandom_idx_list = [random.randint(1, len(df.text)) for i in range(10)] # Creates random indexes to choose from  dataframe\ndf.loc[random_idx_list, :].head(10).style.set_properties(**{'background-color' :'#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'}) #Returns the row with indexes and displays it","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:16.268168Z","iopub.execute_input":"2024-06-21T05:29:16.268525Z","iopub.status.idle":"2024-06-21T05:29:16.284056Z","shell.execute_reply.started":"2024-06-21T05:29:16.268494Z","shell.execute_reply":"2024-06-21T05:29:16.283103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The Data has alot of punctuations that are words that we use without any contextual meaning. So we need to get rid of it.","metadata":{}},{"cell_type":"markdown","source":"# Text Preprocessing","metadata":{}},{"cell_type":"markdown","source":"Tweet texts often consists of other user mentions, hyperlink texts, emoticons and punctuations. In order to use them for learning using a Language Model. We cannot permit those texts for training a model. So we have to clean the text data using various preprocessing and cleansing methods. ","metadata":{}},{"cell_type":"markdown","source":"*Stemming / Lematization / Stopwords*","metadata":{}},{"cell_type":"code","source":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer('english')\n\ntext_cleaning_re = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:16.285573Z","iopub.execute_input":"2024-06-21T05:29:16.285998Z","iopub.status.idle":"2024-06-21T05:29:16.302390Z","shell.execute_reply.started":"2024-06-21T05:29:16.285959Z","shell.execute_reply":"2024-06-21T05:29:16.301484Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def preprocess(text, stem=False):\n    text = re.sub(text_cleaning_re, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:16.303571Z","iopub.execute_input":"2024-06-21T05:29:16.303894Z","iopub.status.idle":"2024-06-21T05:29:16.313636Z","shell.execute_reply.started":"2024-06-21T05:29:16.303864Z","shell.execute_reply":"2024-06-21T05:29:16.312714Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.text = df.text.apply(lambda x :preprocess(x))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:29:16.314784Z","iopub.execute_input":"2024-06-21T05:29:16.315178Z","iopub.status.idle":"2024-06-21T05:30:16.790220Z","shell.execute_reply.started":"2024-06-21T05:29:16.315138Z","shell.execute_reply":"2024-06-21T05:30:16.789422Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The data is clean & tidy. Now lets see the word cloud visualization of it.","metadata":{}},{"cell_type":"markdown","source":"**Positive Words**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20))\nwc =  WordCloud(max_words = 2000, width = 1600, height = 800).generate(\" \".join(df[df.sentiment == 'Positive'].text))\nplt.imshow(wc, interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:30:16.791243Z","iopub.execute_input":"2024-06-21T05:30:16.791509Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Negative Words**","metadata":{}},{"cell_type":"code","source":"from wordcloud import WordCloud\n\nplt.figure(figsize = (20,20))\nwc =  WordCloud(max_words = 2000, width = 1600, height = 800).generate(\" \".join(df[df.sentiment == 'Negative'].text))\nplt.imshow(wc, interpolation = 'bilinear')","metadata":{"execution":{"iopub.status.idle":"2024-06-21T05:32:23.347261Z","shell.execute_reply.started":"2024-06-21T05:31:19.732682Z","shell.execute_reply":"2024-06-21T05:32:23.346166Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Train and Test Split","metadata":{}},{"cell_type":"code","source":"TRAIN_SIZE = 0.8\nMAX_NB_WORDS = 100000\nMAX_SEQUENCE_LENGTH = 30","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:32:23.351182Z","iopub.execute_input":"2024-06-21T05:32:23.351503Z","iopub.status.idle":"2024-06-21T05:32:23.357061Z","shell.execute_reply.started":"2024-06-21T05:32:23.351477Z","shell.execute_reply":"2024-06-21T05:32:23.355861Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_data, test_data =  train_test_split(df, test_size = 1-TRAIN_SIZE, random_state = 7) #Splits Dataset intro Training and Testing set\n\nprint(\"Train Data size:\", len(train_data))\nprint(\"Test Data size:\", len(test_data))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:32:23.358324Z","iopub.execute_input":"2024-06-21T05:32:23.358630Z","iopub.status.idle":"2024-06-21T05:32:23.653393Z","shell.execute_reply.started":"2024-06-21T05:32:23.358595Z","shell.execute_reply":"2024-06-21T05:32:23.652438Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"train_test_split will shuffle the dataset and split it to gives training and testing dataset. It's important to shuffle our dataset before training.","metadata":{}},{"cell_type":"code","source":"train_data.sample(10).style.set_properties(**{'background-color': '#f9f9f9', 'color': '#4CAF50', 'font-weight': 'bold'}) # Returns the rows with the index and display it","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:32:23.654529Z","iopub.execute_input":"2024-06-21T05:32:23.654828Z","iopub.status.idle":"2024-06-21T05:32:23.710112Z","shell.execute_reply.started":"2024-06-21T05:32:23.654800Z","shell.execute_reply":"2024-06-21T05:32:23.709158Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Tokenization**","metadata":{}},{"cell_type":"markdown","source":"Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation. The process is called Tokenization.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data.text)\n\nword_index = tokenizer.word_index\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"Vocabulary size:\", vocab_size)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:32:23.711466Z","iopub.execute_input":"2024-06-21T05:32:23.711869Z","iopub.status.idle":"2024-06-21T05:32:45.433902Z","shell.execute_reply.started":"2024-06-21T05:32:23.711837Z","shell.execute_reply":"2024-06-21T05:32:45.432940Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\nx_train = pad_sequences(tokenizer.texts_to_sequences(train_data.text),\n                      maxlen =  MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokenizer.texts_to_sequences(test_data.text), \n                     maxlen = MAX_SEQUENCE_LENGTH)\n\nprint(\"Training X Shape:\", x_train.shape)\nprint(\"Testing X Shape:\", x_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:32:45.435478Z","iopub.execute_input":"2024-06-21T05:32:45.435821Z","iopub.status.idle":"2024-06-21T05:33:16.011347Z","shell.execute_reply.started":"2024-06-21T05:32:45.435785Z","shell.execute_reply":"2024-06-21T05:33:16.010463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"labels = train_data.sentiment.unique().tolist()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:33:16.012681Z","iopub.execute_input":"2024-06-21T05:33:16.013040Z","iopub.status.idle":"2024-06-21T05:33:16.114052Z","shell.execute_reply.started":"2024-06-21T05:33:16.013006Z","shell.execute_reply":"2024-06-21T05:33:16.113354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Label Encoding**","metadata":{}},{"cell_type":"markdown","source":"We are building the model to predict class in encoded form (0 or 1 as this is a binary classification). We should encode our training labels to encodings","metadata":{}},{"cell_type":"code","source":"encoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())\n\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\n\ny_train = y_train.reshape(-1,1)\ny_test = y_test.reshape(-1,1)\n\nprint(\"y_train_shape:\", y_train.shape)\nprint(\"y_test_shape:\", y_test.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:33:16.114963Z","iopub.execute_input":"2024-06-21T05:33:16.115231Z","iopub.status.idle":"2024-06-21T05:33:18.088303Z","shell.execute_reply.started":"2024-06-21T05:33:16.115207Z","shell.execute_reply":"2024-06-21T05:33:18.087369Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Word Embedding","metadata":{}},{"cell_type":"markdown","source":"In this notebook, I use GloVe Embedding from Stanford AI which can be found [HERE](https://nlp.stanford.edu/projects/glove/)","metadata":{}},{"cell_type":"code","source":"!wget http://nlp.stanford.edu/data/glove.6B.zip\n!unzip glove.6B.zip","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:33:18.089642Z","iopub.execute_input":"2024-06-21T05:33:18.090031Z","iopub.status.idle":"2024-06-21T05:36:19.478971Z","shell.execute_reply.started":"2024-06-21T05:33:18.089994Z","shell.execute_reply":"2024-06-21T05:36:19.477711Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"GLOVE_EMB = '/kaggle/working/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = 1e-3\nBATCH_SIZE = 1024\nEPOCHS = 50\nMODEL_PATH = '.../output/kaggle/working/best_model.hdf5'","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:19.480568Z","iopub.execute_input":"2024-06-21T05:36:19.480884Z","iopub.status.idle":"2024-06-21T05:36:19.486245Z","shell.execute_reply.started":"2024-06-21T05:36:19.480852Z","shell.execute_reply":"2024-06-21T05:36:19.485328Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embeddings_index = {}\n\nf = open(GLOVE_EMB)\nfor line in f:\n    values = line.split()\n    word = value = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' %len(embeddings_index))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:19.487523Z","iopub.execute_input":"2024-06-21T05:36:19.487791Z","iopub.status.idle":"2024-06-21T05:36:51.962644Z","shell.execute_reply.started":"2024-06-21T05:36:19.487767Z","shell.execute_reply":"2024-06-21T05:36:51.961667Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[i] = embedding_vector","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:51.963858Z","iopub.execute_input":"2024-06-21T05:36:51.964151Z","iopub.status.idle":"2024-06-21T05:36:52.519600Z","shell.execute_reply.started":"2024-06-21T05:36:51.964126Z","shell.execute_reply":"2024-06-21T05:36:52.518771Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"embedding_layer = tf.keras.layers.Embedding(vocab_size,\n                                          EMBEDDING_DIM,\n                                          weights=[embedding_matrix],\n                                          input_length=MAX_SEQUENCE_LENGTH,\n                                          trainable=False)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:52.520746Z","iopub.execute_input":"2024-06-21T05:36:52.521047Z","iopub.status.idle":"2024-06-21T05:36:54.605533Z","shell.execute_reply.started":"2024-06-21T05:36:52.521022Z","shell.execute_reply":"2024-06-21T05:36:54.604502Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Model Training LSTM** ","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout\nfrom tensorflow.keras.layers import SpatialDropout1D\nfrom tensorflow.keras.callbacks import ModelCheckpoint","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:54.606851Z","iopub.execute_input":"2024-06-21T05:36:54.607261Z","iopub.status.idle":"2024-06-21T05:36:54.618359Z","shell.execute_reply.started":"2024-06-21T05:36:54.607224Z","shell.execute_reply":"2024-06-21T05:36:54.617554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.2, recurrent_dropout=0.2))(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.5)(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\nmodel = tf.keras.Model(sequence_input, outputs)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:54.619516Z","iopub.execute_input":"2024-06-21T05:36:54.619821Z","iopub.status.idle":"2024-06-21T05:36:55.630853Z","shell.execute_reply.started":"2024-06-21T05:36:54.619796Z","shell.execute_reply":"2024-06-21T05:36:55.630048Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model.summary()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:55.631989Z","iopub.execute_input":"2024-06-21T05:36:55.632284Z","iopub.status.idle":"2024-06-21T05:36:55.657027Z","shell.execute_reply.started":"2024-06-21T05:36:55.632259Z","shell.execute_reply":"2024-06-21T05:36:55.656222Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\n\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',\n              metrics=['accuracy'])\nReduceLROnPlateau = ReduceLROnPlateau(factor=0.1,\n                                     min_lr = 0.01,\n                                     monitor = 'val_loss',\n                                     verbose = 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:55.658099Z","iopub.execute_input":"2024-06-21T05:36:55.658530Z","iopub.status.idle":"2024-06-21T05:36:55.677304Z","shell.execute_reply.started":"2024-06-21T05:36:55.658506Z","shell.execute_reply":"2024-06-21T05:36:55.676427Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(\"Training on GPU...\") if tf.test.is_gpu_available() else print(\"Training on CPU...\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:55.678427Z","iopub.execute_input":"2024-06-21T05:36:55.678692Z","iopub.status.idle":"2024-06-21T05:36:55.686322Z","shell.execute_reply.started":"2024-06-21T05:36:55.678669Z","shell.execute_reply":"2024-06-21T05:36:55.685489Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"history = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS,\n                    validation_data=(x_test, y_test), callbacks=[ReduceLROnPlateau])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T05:36:55.687436Z","iopub.execute_input":"2024-06-21T05:36:55.687749Z","iopub.status.idle":"2024-06-21T06:33:15.714704Z","shell.execute_reply.started":"2024-06-21T05:36:55.687719Z","shell.execute_reply":"2024-06-21T06:33:15.713781Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Evaluation ","metadata":{}},{"cell_type":"markdown","source":"Now that we have trained the model, we can evaluate its performance. We will some evaluation metrics and techniques to test the model.\n\nLet's start with the Learning Curve of loss and accuracy of the model on each epoch.","metadata":{}},{"cell_type":"code","source":"s, (at, al) = plt.subplots(2,1)\nat.plot(history.history['accuracy'], c= 'b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc = 'upper left')","metadata":{"execution":{"iopub.status.busy":"2024-06-21T06:33:15.716432Z","iopub.execute_input":"2024-06-21T06:33:15.716739Z","iopub.status.idle":"2024-06-21T06:33:16.288556Z","shell.execute_reply.started":"2024-06-21T06:33:15.716711Z","shell.execute_reply":"2024-06-21T06:33:16.287565Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The model will output a prediction score between 0 and 1. We can classify two classes by defining a threshold value for it. In our case, I have set 0.5 as THRESHOLD value, if the score above it. Then it will be classified as POSITIVE sentiment.","metadata":{}},{"cell_type":"code","source":"def decode_sentiment(score):\n    return \"Positive\" if score>0.5 else \"Negative\"\n\n\nscores = model.predict(x_test, verbose=1, batch_size=10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","metadata":{"execution":{"iopub.status.busy":"2024-06-21T06:39:51.816303Z","iopub.execute_input":"2024-06-21T06:39:51.817109Z","iopub.status.idle":"2024-06-21T06:39:56.255154Z","shell.execute_reply.started":"2024-06-21T06:39:51.817075Z","shell.execute_reply":"2024-06-21T06:39:56.253951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Confusion Matrix ","metadata":{}},{"cell_type":"markdown","source":"Confusion Matrix provide a nice overlook at the model's performance in classification task","metadata":{}},{"cell_type":"code","source":"import itertools\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\ndef plot_confusion_matrix(cm, classes,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T06:42:06.538454Z","iopub.execute_input":"2024-06-21T06:42:06.539413Z","iopub.status.idle":"2024-06-21T06:42:06.549255Z","shell.execute_reply.started":"2024-06-21T06:42:06.539377Z","shell.execute_reply":"2024-06-21T06:42:06.548241Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"cnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6,6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T06:42:25.878662Z","iopub.execute_input":"2024-06-21T06:42:25.879060Z","iopub.status.idle":"2024-06-21T06:42:27.564947Z","shell.execute_reply.started":"2024-06-21T06:42:25.879027Z","shell.execute_reply":"2024-06-21T06:42:27.563935Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Classification Score","metadata":{}},{"cell_type":"code","source":"print(classification_report(list(test_data.sentiment), y_pred_1d))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T06:43:31.554377Z","iopub.execute_input":"2024-06-21T06:43:31.554769Z","iopub.status.idle":"2024-06-21T06:43:37.255497Z","shell.execute_reply.started":"2024-06-21T06:43:31.554738Z","shell.execute_reply":"2024-06-21T06:43:37.254307Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It's a pretty good model we trained here in terms of NLP. Around 80% accuracy is good enough considering the baseline human accuracy also pretty low in these tasks. Also, you may go on and explore the dataset, some tweets might have other languages than English. So our Embedding and Tokenizing wont have effect on them. But on practical scenario, this model is good for handling most tasks for Sentiment Analysis.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}